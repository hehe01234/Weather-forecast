import sys
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import datetime
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
import time

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class WeatherScraper:
    def __init__(self):
        # å¤šä¸ªæ•°æ®æºé…ç½®
        self.data_sources = {
            'open_meteo': {
                'name': 'Open-Meteo',
                'url': 'https://archive-api.open-meteo.com/v1/archive',
                'free': True,
                'description': 'å…è´¹å¼€æºå¤©æ°”API'
            },
            'openweather': {
                'name': 'OpenWeatherMap',
                'url': 'https://history.openweathermap.org/data/2.5/history/city',
                'free': False,  # éœ€è¦API key
                'description': 'å…¨çƒçŸ¥åå¤©æ°”APIï¼ˆéœ€è¦API keyï¼‰'
            },
            'weatherapi': {
                'name': 'WeatherAPI',
                'url': 'http://api.weatherapi.com/v1/history.json',
                'free': False,  # éœ€è¦API key
                'description': 'ä¸“ä¸šå¤©æ°”APIï¼ˆéœ€è¦API keyï¼‰'
            },
            'visual_crossing': {
                'name': 'Visual Crossing',
                'url': 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline',
                'free': False,  # éœ€è¦API key
                'description': 'å¯è§†åŒ–å¤©æ°”æ•°æ®APIï¼ˆéœ€è¦API keyï¼‰'
            }
        }
        
        # API Keys (ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œé…ç½®è‡ªå·±çš„APIå¯†é’¥)
        self.api_keys = {
            'openweather': '',  # åœ¨è¿™é‡Œå¡«å…¥OpenWeatherMapçš„API key
            'weatherapi': '',   # åœ¨è¿™é‡Œå¡«å…¥WeatherAPIçš„API key
            'visual_crossing': ''  # åœ¨è¿™é‡Œå¡«å…¥Visual Crossingçš„API key
        }

    def get_city_coordinates(self, city_name):
        city_coords = {
            "åŒ—äº¬": (39.90, 116.41),
            "ä¸Šæµ·": (31.23, 121.47),
            "å¹¿å·": (23.13, 113.26),
            "æ·±åœ³": (22.54, 114.05),
            "è¥¿å®‰": (34.34, 108.94),
        }
        return city_coords.get(city_name, None)

    def fetch_weather_data(self, city_name, days_back=1825):
        coords = self.get_city_coordinates(city_name)
        if not coords:
            raise ValueError(f"ä¸æ”¯æŒçš„åŸå¸‚ï¼š{city_name}")

        print(f"ğŸŒ æ­£åœ¨ä¸º {city_name} è·å–å¤©æ°”æ•°æ®...")
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒæ•°æ®æº
        data_source_priority = ['open_meteo', 'weatherapi', 'visual_crossing', 'openweather']
        
        for source_name in data_source_priority:
            source_info = self.data_sources[source_name]
            print(f"\nğŸ“¡ å°è¯•æ•°æ®æº: {source_info['name']} ({source_info['description']})")
            
            try:
                if source_name == 'open_meteo':
                    df = self._fetch_from_open_meteo(city_name, coords, days_back)
                elif source_name == 'weatherapi':
                    df = self._fetch_from_weatherapi(city_name, coords, days_back)
                elif source_name == 'visual_crossing':
                    df = self._fetch_from_visual_crossing(city_name, coords, days_back)
                elif source_name == 'openweather':
                    df = self._fetch_from_openweather(city_name, coords, days_back)
                
                if df is not None and len(df) > 100:  # ç¡®ä¿è·å–åˆ°è¶³å¤Ÿæ•°æ®
                    print(f"âœ… æˆåŠŸä» {source_info['name']} è·å– {len(df)} å¤©çš„æ•°æ®")
                    self._save_cache(df, city_name)
                    return df
                else:
                    print(f"âŒ {source_info['name']} è¿”å›æ•°æ®ä¸è¶³")
                    
            except Exception as e:
                print(f"âŒ {source_info['name']} è·å–å¤±è´¥: {str(e)}")
                continue
        
        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°ç¼“å­˜æˆ–ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("\nğŸ”„ æ‰€æœ‰åœ¨çº¿æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        return self._try_fallback_data(city_name, days_back)

    def _fetch_from_open_meteo(self, city_name, coords, days_back):
        """ä»Open-Meteoè·å–æ•°æ®"""
        latitude, longitude = coords
        end_date = datetime.datetime.now()
        start_date = end_date - datetime.timedelta(days=days_back)

        params = {
            "latitude": latitude,
            "longitude": longitude,
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d"),
            "hourly": "temperature_2m,precipitation,surface_pressure,windspeed_10m,relative_humidity_2m,cloudcover",
            "daily": "temperature_2m_max,temperature_2m_min,precipitation_sum"
        }

        url = self.data_sources['open_meteo']['url']
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(3):
            try:
                print(f"  ğŸ”„ å°è¯•è¿æ¥ Open-Meteo (ç¬¬ {attempt + 1}/3 æ¬¡)...")
                response = requests.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    return self._process_open_meteo_data(data)
                else:
                    print(f"  âŒ HTTPé”™è¯¯: {response.status_code}")
                    
            except requests.exceptions.RequestException as e:
                print(f"  âŒ è¿æ¥é”™è¯¯: {e}")
                if attempt < 2:
                    time.sleep(5 * (attempt + 1))
        
        return None

    def _fetch_from_weatherapi(self, city_name, coords, days_back):
        """ä»WeatherAPIè·å–æ•°æ®"""
        if not self.api_keys.get('weatherapi'):
            print("  âš ï¸ WeatherAPIéœ€è¦API keyï¼Œè·³è¿‡")
            return None
            
        latitude, longitude = coords
        # WeatherAPIçš„å†å²æ•°æ®è·å–æ–¹æ³•
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨éœ€è¦API key
        print("  âš ï¸ WeatherAPIéœ€è¦æœ‰æ•ˆçš„API keyæ‰èƒ½ä½¿ç”¨")
        return None

    def _fetch_from_visual_crossing(self, city_name, coords, days_back):
        """ä»Visual Crossingè·å–æ•°æ®"""
        if not self.api_keys.get('visual_crossing'):
            print("  âš ï¸ Visual Crossingéœ€è¦API keyï¼Œè·³è¿‡")
            return None
            
        # Visual Crossingçš„APIè°ƒç”¨ç¤ºä¾‹
        print("  âš ï¸ Visual Crossingéœ€è¦æœ‰æ•ˆçš„API keyæ‰èƒ½ä½¿ç”¨")
        return None

    def _fetch_from_openweather(self, city_name, coords, days_back):
        """ä»OpenWeatherMapè·å–æ•°æ®"""
        if not self.api_keys.get('openweather'):
            print("  âš ï¸ OpenWeatherMapéœ€è¦API keyï¼Œè·³è¿‡")
            return None
            
        # OpenWeatherMapçš„å†å²æ•°æ®APIè°ƒç”¨
        print("  âš ï¸ OpenWeatherMapéœ€è¦æœ‰æ•ˆçš„API keyæ‰èƒ½ä½¿ç”¨")
        return None

    def _process_open_meteo_data(self, data):
        """å¤„ç†Open-Meteoæ•°æ®æ ¼å¼"""
        # ä½¿ç”¨æ¯æ—¥æ•°æ®è·å–æ›´å‡†ç¡®çš„æœ€é«˜æœ€ä½æ¸©åº¦
        daily_df = pd.DataFrame({
            "date": pd.to_datetime(data["daily"]["time"]),
            "high_temp": data["daily"]["temperature_2m_max"],
            "low_temp": data["daily"]["temperature_2m_min"],
            "rain": data["daily"]["precipitation_sum"]
        })

        # ä»å°æ—¶æ•°æ®è·å–å…¶ä»–ç‰¹å¾çš„æ—¥å¹³å‡å€¼
        hourly_df = pd.DataFrame({
            "datetime": pd.to_datetime(data["hourly"]["time"]),
            "pressure": data["hourly"]["surface_pressure"],
            "wind_speed": data["hourly"]["windspeed_10m"],
            "humidity": data["hourly"]["relative_humidity_2m"],
            "cloudcover": data["hourly"]["cloudcover"]
        })

        # æŒ‰æ—¥æœŸèšåˆå°æ—¶æ•°æ®
        hourly_df['date'] = hourly_df['datetime'].dt.date
        hourly_agg = hourly_df.groupby('date').agg({
            "pressure": "mean",
            "wind_speed": "mean",
            "humidity": "mean",
            "cloudcover": "mean"
        }).reset_index()
        hourly_agg['date'] = pd.to_datetime(hourly_agg['date'])

        # åˆå¹¶æ—¥æ•°æ®å’Œå°æ—¶èšåˆæ•°æ®
        df = pd.merge(daily_df, hourly_agg, on='date', how='inner')
        
        # è®¡ç®—å¹³å‡æ¸©åº¦
        df['avg_temp'] = (df['high_temp'] + df['low_temp']) / 2

        return df

    def _try_fallback_data(self, city_name, days_back):
        """å°è¯•ä½¿ç”¨å¤‡ç”¨æ•°æ®æºæˆ–æœ¬åœ°ç¼“å­˜"""
        # é¦–å…ˆå°è¯•è¯»å–æœ¬åœ°ç¼“å­˜
        cache_file = f"weather_cache_{city_name}.csv"
        if os.path.exists(cache_file):
            print(f"ğŸ“ æ‰¾åˆ°æœ¬åœ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
            try:
                df = pd.read_csv(cache_file)
                df['date'] = pd.to_datetime(df['date'])
                print(f"âœ… æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ {len(df)} å¤©çš„æ•°æ®")
                return df
            except Exception as e:
                print(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        # å°è¯•å…è´¹çš„å…¬å…±æ•°æ®æº
        print("ğŸŒ å°è¯•å…è´¹çš„å…¬å…±å¤©æ°”æ•°æ®æº...")
        
        # å°è¯•MeteoStat (å¦ä¸€ä¸ªå…è´¹çš„å¤©æ°”æ•°æ®æº)
        try:
            df = self._fetch_from_meteostat(city_name, days_back)
            if df is not None:
                return df
        except Exception as e:
            print(f"âŒ MeteoStatè·å–å¤±è´¥: {e}")
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        print("ğŸ”§ æ‰€æœ‰æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•...")
        return self._generate_mock_data(city_name, days_back)

    def _fetch_from_meteostat(self, city_name, days_back):
        """å°è¯•ä»MeteoStatè·å–æ•°æ®ï¼ˆå…è´¹çš„æ°”è±¡æ•°æ®åº“ï¼‰"""
        try:
            # æ³¨æ„ï¼šè¿™éœ€è¦å®‰è£…meteostatåº“: pip install meteostat
            import meteostat
            from meteostat import Point, Daily
            
            coords = self.get_city_coordinates(city_name)
            latitude, longitude = coords
            
            end_date = datetime.datetime.now()
            start_date = end_date - datetime.timedelta(days=days_back)
            
            print("  ğŸ”„ å°è¯•è¿æ¥ MeteoStat...")
            
            # åˆ›å»ºåœ°ç‚¹å¯¹è±¡
            location = Point(latitude, longitude)
            
            # è·å–æ¯æ—¥æ•°æ®
            data = Daily(location, start_date, end_date)
            data = data.fetch()
            
            if len(data) > 100:
                df = pd.DataFrame({
                    'date': data.index,
                    'high_temp': data['tmax'],
                    'low_temp': data['tmin'],
                    'avg_temp': data['tavg'],
                    'rain': data['prcp'].fillna(0),
                    'pressure': data['pres'].fillna(1013),
                    'wind_speed': data['wspd'].fillna(0),
                    'humidity': data.get('rhum', pd.Series([50] * len(data))),  # é»˜è®¤æ¹¿åº¦50%
                    'cloudcover': data.get('tsun', pd.Series([50] * len(data)))  # ç”¨æ—¥ç…§æ—¶é—´ä¼°ç®—äº‘é‡
                })
                
                # å¡«å……ç¼ºå¤±çš„avg_temp
                df['avg_temp'] = df['avg_temp'].fillna((df['high_temp'] + df['low_temp']) / 2)
                
                # é‡ç½®ç´¢å¼•
                df = df.reset_index(drop=True)
                
                print(f"âœ… MeteoStat æˆåŠŸè·å– {len(df)} å¤©çš„æ•°æ®")
                return df
            else:
                print("âŒ MeteoStat æ•°æ®ä¸è¶³")
                return None
                
        except ImportError:
            print("  âš ï¸ MeteoStatåº“æœªå®‰è£…ï¼Œè¿è¡Œ: pip install meteostat")
            return None
        except Exception as e:
            print(f"  âŒ MeteoStaté”™è¯¯: {e}")
            return None

    def _save_cache(self, df, city_name):
        """ä¿å­˜æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜"""
        cache_file = f"weather_cache_{city_name}.csv"
        try:
            df.to_csv(cache_file, index=False)
            print(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _generate_mock_data(self, city_name, days_back):
        """ç”Ÿæˆæ¨¡æ‹Ÿå¤©æ°”æ•°æ®ç”¨äºæµ‹è¯•"""
        print("ğŸ­ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿå¤©æ°”æ•°æ®...")
        
        end_date = datetime.datetime.now()
        start_date = end_date - datetime.timedelta(days=days_back)
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ—
        dates = pd.date_range(start=start_date, end=end_date, freq='D')[:-1]  # å»æ‰æœ€åä¸€å¤©
        
        # æ ¹æ®åŸå¸‚è®¾ç½®åŸºç¡€æ¸©åº¦
        city_base_temps = {
            "åŒ—äº¬": 15, "ä¸Šæµ·": 18, "å¹¿å·": 22, "æ·±åœ³": 24, "è¥¿å®‰": 16
        }
        base_temp = city_base_temps.get(city_name, 18)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)  # å›ºå®šéšæœºç§å­
        n_days = len(dates)
        
        # æ¸©åº¦æ•°æ®ï¼ˆè€ƒè™‘å­£èŠ‚æ€§ï¼‰
        seasonal_temp = 10 * np.sin(2 * np.pi * np.arange(n_days) / 365.25 - np.pi/2)
        daily_variation = np.random.normal(0, 3, n_days)
        avg_temps = base_temp + seasonal_temp + daily_variation
        
        high_temps = avg_temps + np.random.uniform(3, 8, n_days)
        low_temps = avg_temps - np.random.uniform(3, 8, n_days)
        
        # é™é›¨æ•°æ®ï¼ˆåå‘å°é›¨å’Œæ— é›¨ï¼‰
        rain_prob = np.random.random(n_days)
        rain_amounts = np.where(rain_prob < 0.7, 0, np.random.exponential(2, n_days))  # 70%æ— é›¨
        
        # å…¶ä»–æ°”è±¡æ•°æ®
        pressures = np.random.normal(1013, 20, n_days)
        wind_speeds = np.random.exponential(3, n_days)
        humidities = np.random.uniform(30, 90, n_days)
        cloudcovers = np.random.uniform(0, 100, n_days)
        
        df = pd.DataFrame({
            'date': dates,
            'high_temp': high_temps,
            'low_temp': low_temps,
            'avg_temp': (high_temps + low_temps) / 2,
            'rain': rain_amounts,
            'pressure': pressures,
            'wind_speed': wind_speeds,
            'humidity': humidities,
            'cloudcover': cloudcovers
        })
        
        print(f"âœ… ç”Ÿæˆäº† {len(df)} å¤©çš„æ¨¡æ‹Ÿæ•°æ®")
        print("âš ï¸ æ³¨æ„: è¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œä»…ç”¨äºæµ‹è¯•ç³»ç»ŸåŠŸèƒ½")
        
        return df

    def list_available_sources(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        print("\nğŸŒ å¯ç”¨çš„å¤©æ°”æ•°æ®æº:")
        print("=" * 60)
        
        for source_name, info in self.data_sources.items():
            status = "âœ… å…è´¹" if info['free'] else "ğŸ”‘ éœ€è¦API key"
            has_key = "âœ… å·²é…ç½®" if self.api_keys.get(source_name) else "âŒ æœªé…ç½®"
            
            print(f"{info['name']:20} | {status:10} | {has_key:10} | {info['description']}")
        
        print("\nğŸ’¡ æç¤º:")
        print("1. Open-Meteo æ˜¯å®Œå…¨å…è´¹çš„ï¼Œæ— éœ€API key")
        print("2. å…¶ä»–æœåŠ¡éœ€è¦æ³¨å†Œå¹¶è·å–API key")
        print("3. å¤§å¤šæ•°æœåŠ¡éƒ½æœ‰å…è´¹é¢åº¦ï¼Œè¶³å¤Ÿä¸ªäººä½¿ç”¨")
        print("4. é…ç½®å¤šä¸ªæ•°æ®æºå¯ä»¥æé«˜ç³»ç»Ÿå¯é æ€§")

    def set_api_key(self, source_name, api_key):
        """è®¾ç½®APIå¯†é’¥"""
        if source_name in self.api_keys:
            self.api_keys[source_name] = api_key
            print(f"âœ… å·²è®¾ç½® {source_name} çš„API key")
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æº: {source_name}")

class WeatherDataProcessor:
    def __init__(self):
        self.scaler = StandardScaler()

    def preprocess_data(self, df, city="æœªçŸ¥åŸå¸‚", enable_visualization=False):
        processed_df = df.copy()
        processed_df['date'] = pd.to_datetime(processed_df['date'])
        processed_df['year'] = processed_df['date'].dt.year
        processed_df['month'] = processed_df['date'].dt.month
        processed_df['day'] = processed_df['date'].dt.day
        processed_df['day_of_week'] = processed_df['date'].dt.dayofweek
        
        # æ·»åŠ æ›´å¤šç‰¹å¾
        processed_df['season'] = processed_df['month'].apply(self._get_season)
        processed_df['weekend_flag'] = (processed_df['day_of_week'] >= 5).astype(int)
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡ç‰¹å¾
        processed_df['temp_ma_3'] = processed_df['avg_temp'].rolling(window=3, min_periods=1).mean()
        processed_df['temp_ma_7'] = processed_df['avg_temp'].rolling(window=7, min_periods=1).mean()
        processed_df['rain_ma_3'] = processed_df['rain'].rolling(window=3, min_periods=1).mean()
        
        # æ·»åŠ è¶‹åŠ¿ç‰¹å¾
        processed_df['temp_trend'] = processed_df['avg_temp'].diff().fillna(0)
        processed_df['pressure_change'] = processed_df['pressure'].diff().fillna(0)
        processed_df['humidity_change'] = processed_df['humidity'].diff().fillna(0)
        
        processed_df.dropna(inplace=True)
        return processed_df
    
    def _get_season(self, month):
        if month in [12, 1, 2]:
            return 0  # å†¬å­£
        elif month in [3, 4, 5]:
            return 1  # æ˜¥å­£
        elif month in [6, 7, 8]:
            return 2  # å¤å­£
        else:
            return 3  # ç§‹å­£

    def create_sequences(self, data, feature_cols, target_cols, seq_length=7, forecast_days=3):
        """æ”¯æŒå¤šç›®æ ‡è¾“å‡ºçš„åºåˆ—åˆ›å»º"""
        X, y = [], []
        for i in range(len(data) - seq_length - forecast_days):
            X.append(data[feature_cols].iloc[i:i + seq_length].values)
            if isinstance(target_cols, list):
                # å¤šç›®æ ‡è¾“å‡º
                y.append(data[target_cols].iloc[i + seq_length:i + seq_length + forecast_days].values)
            else:
                # å•ç›®æ ‡è¾“å‡º
                y.append(data[target_cols].iloc[i + seq_length:i + seq_length + forecast_days].values)
        return np.array(X), np.array(y)

    def split_train_val_test_by_time(self, X, y, df, seq_length=7, forecast_days=3):
        val_start_date = df['date'].max() - pd.DateOffset(years=2)
        test_start_date = df['date'].max() - pd.DateOffset(years=1)

        val_indices = df[(df['date'] >= val_start_date) & (df['date'] < test_start_date)].index
        test_indices = df[df['date'] >= test_start_date].index

        train_end_idx = val_indices.min() - seq_length - forecast_days
        val_end_idx = test_indices.min() - seq_length - forecast_days

        X_train = X[:train_end_idx]
        y_train = y[:train_end_idx]
        X_val = X[train_end_idx:val_end_idx]
        y_val = y[train_end_idx:val_end_idx]
        X_test = X[val_end_idx:]
        y_test = y[val_end_idx:]

        print("\n=== æ•°æ®åˆ’åˆ†è¯¦æƒ… ===")
        print(f"æ€»æ ·æœ¬æ•°: {len(X)}")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(X_val)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test)}")
        print(f"éªŒè¯é›†èµ·å§‹æ—¥æœŸ: {val_start_date.date()}")
        if len(X_val) > 0:
            print(f"éªŒè¯é›†è¦†ç›–æ—¶é—´èŒƒå›´: {df.iloc[train_end_idx]['date']} åˆ° {df.iloc[val_end_idx - 1]['date']}")
        if len(X_test) > 0:
            print(f"æµ‹è¯•é›†èµ·å§‹æ—¥æœŸ: {test_start_date.date()}")
            print(f"æµ‹è¯•é›†è¦†ç›–æ—¶é—´èŒƒå›´: {df.iloc[val_end_idx]['date']} åˆ° {df.iloc[-forecast_days - 1]['date']}")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def scale_data(self, X_train, X_val, X_test):
        n_samples_train, n_steps, n_features = X_train.shape
        n_samples_val = X_val.shape[0] if len(X_val) > 0 else 0
        n_samples_test = X_test.shape[0] if len(X_test) > 0 else 0

        X_train_flat = X_train.reshape(-1, n_features)
        X_train_scaled = self.scaler.fit_transform(X_train_flat).reshape(n_samples_train, n_steps, n_features)
        
        if n_samples_val > 0:
            X_val_flat = X_val.reshape(-1, n_features)
            X_val_scaled = self.scaler.transform(X_val_flat).reshape(n_samples_val, n_steps, n_features)
        else:
            X_val_scaled = np.array([])
            
        if n_samples_test > 0:
            X_test_flat = X_test.reshape(-1, n_features)
            X_test_scaled = self.scaler.transform(X_test_flat).reshape(n_samples_test, n_steps, n_features)
        else:
            X_test_scaled = np.array([])

        return X_train_scaled, X_val_scaled, X_test_scaled

# å¤šè¾“å‡ºæ¸©åº¦é¢„æµ‹æ¨¡å‹
class MultiOutputTemperaturePredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, forecast_days, num_layers=3, dropout=0.4):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)
        self.attention = nn.MultiheadAttention(hidden_dim*2, num_heads=8, dropout=dropout)
        self.norm1 = nn.LayerNorm(hidden_dim*2)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # ä¸‰ä¸ªè¾“å‡ºåˆ†æ”¯ï¼šæœ€é«˜æ¸©ã€æœ€ä½æ¸©ã€å¹³å‡æ¸©
        self.fc_high = nn.Sequential(
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, forecast_days)
        )
        
        self.fc_low = nn.Sequential(
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, forecast_days)
        )
        
        self.fc_avg = nn.Sequential(
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, forecast_days)
        )

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        lstm_out_t = lstm_out.transpose(0, 1)
        attn_out, _ = self.attention(lstm_out_t, lstm_out_t, lstm_out_t)
        attn_out = attn_out.transpose(0, 1)
        attn_out = self.norm1(attn_out + lstm_out)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_hidden = attn_out[:, -1, :]
        
        # ä¸‰ä¸ªåˆ†æ”¯è¾“å‡º
        high_temp = self.fc_high(last_hidden)
        low_temp = self.fc_low(last_hidden)
        avg_temp = self.fc_avg(last_hidden)
        
        return {
            'high_temp': high_temp,
            'low_temp': low_temp,
            'avg_temp': avg_temp
        }

# ç®€åŒ–çš„é™é›¨é¢„æµ‹æ¨¡å‹
class SimpleRainfallPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, forecast_days, num_layers=2, dropout=0.3):
        super().__init__()
        
        # CNNç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dropout=dropout),
            num_layers=num_layers
        )
        
        # ç›´æ¥é¢„æµ‹é™é›¨é‡ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        self.rain_regressor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, forecast_days),
            nn.ReLU()  # ç¡®ä¿è¾“å‡ºéè´Ÿ
        )

    def forward(self, x):
        # ç‰¹å¾æå–
        x_t = x.transpose(1, 2)
        features = self.feature_extractor(x_t)
        features = features.transpose(1, 2)
        
        # Transformerç¼–ç 
        encoded_features = self.transformer(features)
        last_hidden = encoded_features[:, -1, :]
        
        # ç›´æ¥é¢„æµ‹é™é›¨é‡
        rain_amount = self.rain_regressor(last_hidden)
        
        return rain_amount

# ä¿æŒå‘åå…¼å®¹çš„ä¸¤é˜¶æ®µæ¨¡å‹ï¼ˆé‡å‘½åï¼‰
class TwoStageRainfallPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, forecast_days, num_layers=2, dropout=0.3):
        super().__init__()
        
        # å…±äº«çš„ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dropout=dropout),
            num_layers=num_layers
        )
        
        # ç¬¬ä¸€é˜¶æ®µï¼šé¢„æµ‹æ˜¯å¦ä¼šä¸‹é›¨ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        self.rain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, forecast_days),
            nn.Sigmoid()  # è¾“å‡º0-1ä¹‹é—´çš„æ¦‚ç‡
        )
        
        # ç¬¬äºŒé˜¶æ®µï¼šé¢„æµ‹é™é›¨é‡ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        self.rain_regressor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, forecast_days),
            nn.ReLU()  # ç¡®ä¿è¾“å‡ºéè´Ÿ
        )

    def forward(self, x):
        # ç‰¹å¾æå–
        x_t = x.transpose(1, 2)
        features = self.feature_extractor(x_t)
        features = features.transpose(1, 2)
        
        # Transformerç¼–ç 
        encoded_features = self.transformer(features)
        last_hidden = encoded_features[:, -1, :]
        
        # ä¸¤é˜¶æ®µé¢„æµ‹
        rain_prob = self.rain_classifier(last_hidden)  # é™é›¨æ¦‚ç‡
        rain_amount = self.rain_regressor(last_hidden)  # é™é›¨é‡
        
        return {
            'rain_prob': rain_prob,
            'rain_amount': rain_amount
        }

# ä¿æŒåŸæœ‰çš„æ¨¡å‹ä»¥å‘åå…¼å®¹
class EnhancedLSTMPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.4):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.norm(out[:, -1, :])
        out = self.dropout(out)
        out = self.fc(out)
        return out

class RainfallPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.3):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.ReLU()
        )
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dropout=dropout),
            num_layers=num_layers
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )

    def forward(self, x):
        x = x.transpose(1, 2)
        x = self.cnn(x)
        x = x.transpose(1, 2)
        x = self.transformer(x)
        out = self.fc(x[:, -1, :])
        return out

class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

def create_model_directory(city):
    """ä¸ºæŒ‡å®šåŸå¸‚åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•"""
    model_dir = f"models/{city}"
    os.makedirs(model_dir, exist_ok=True)
    return model_dir

def train_and_save(city, forecast_days_list=[3, 5]):
    scraper = WeatherScraper()
    processor = WeatherDataProcessor()
    df = scraper.fetch_weather_data(city)
    df = processor.preprocess_data(df, city=city)
    
    # æ›´æ–°ç‰¹å¾åˆ—è¡¨ï¼ŒåŒ…å«æ–°ç‰¹å¾
    feature_cols = [
        'year', 'month', 'day', 'day_of_week', 'pressure', 'wind_speed', 
        'humidity', 'cloudcover', 'season', 'weekend_flag',
        'temp_ma_3', 'temp_ma_7', 'rain_ma_3',
        'temp_trend', 'pressure_change', 'humidity_change'
    ]
    
    # åˆ›å»ºåŸå¸‚ç‰¹å®šçš„æ¨¡å‹ç›®å½•
    model_dir = create_model_directory(city)
    
    seq_length = 7

    for forecast_days in forecast_days_list:
        # è®­ç»ƒå¤šè¾“å‡ºæ¸©åº¦é¢„æµ‹æ¨¡å‹
        print(f"\n=== æ­£åœ¨ä¸º{city}è®­ç»ƒå¤šè¾“å‡ºæ¸©åº¦æ¨¡å‹, é¢„æµ‹{forecast_days}å¤© ===")
        temp_targets = ['high_temp', 'low_temp', 'avg_temp']
        X_temp, y_temp = processor.create_sequences(df, feature_cols, temp_targets, seq_length, forecast_days)
        X_train, X_val, X_test, y_train, y_val, y_test = processor.split_train_val_test_by_time(
            X_temp, y_temp, df, seq_length, forecast_days
        )
        X_train_scaled, X_val_scaled, X_test_scaled = processor.scale_data(X_train, X_val, X_test)
        
        # è®­ç»ƒæ¸©åº¦æ¨¡å‹
        input_dim = X_train_scaled.shape[2]
        temp_model = MultiOutputTemperaturePredictor(
            input_dim=input_dim,
            hidden_dim=64,
            forecast_days=forecast_days,
            num_layers=2,
            dropout=0.3
        )
        
        # è®­ç»ƒè¿‡ç¨‹ - æ¸©åº¦æ¨¡å‹
        temp_model = train_multi_output_model(
            temp_model, X_train_scaled, y_train, X_val_scaled, y_val, 
            model_path=f"{model_dir}/temp_model_{forecast_days}days.pth",
            model_type="temperature"
        )
        
        # è®­ç»ƒç®€åŒ–çš„é™é›¨é¢„æµ‹æ¨¡å‹
        print(f"\n=== æ­£åœ¨ä¸º{city}è®­ç»ƒç®€åŒ–é™é›¨æ¨¡å‹, é¢„æµ‹{forecast_days}å¤© ===")
        # ä¸ºé™é›¨æ¨¡å‹å‡†å¤‡æ•°æ®
        rain_target = 'rain'
        X_rain, y_rain = processor.create_sequences(df, feature_cols, rain_target, seq_length, forecast_days)
        X_train_r, X_val_r, X_test_r, y_train_r, y_val_r, y_test_r = processor.split_train_val_test_by_time(
            X_rain, y_rain, df, seq_length, forecast_days
        )
        X_train_r_scaled, X_val_r_scaled, X_test_r_scaled = processor.scale_data(X_train_r, X_val_r, X_test_r)
        
        rain_model = SimpleRainfallPredictor(
            input_dim=input_dim,
            hidden_dim=64,
            forecast_days=forecast_days,
            num_layers=2,
            dropout=0.3
        )
        
        # è®­ç»ƒè¿‡ç¨‹ - é™é›¨æ¨¡å‹
        rain_model = train_simple_rain_model(
            rain_model, X_train_r_scaled, y_train_r, X_val_r_scaled, y_val_r,
            model_path=f"{model_dir}/rain_model_{forecast_days}days.pth"
        )
        
        print(f"åŸå¸‚ {city} çš„æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜åˆ° {model_dir}")

def train_multi_output_model(model, X_train, y_train, X_val, y_val, model_path, model_type="temperature"):
    """è®­ç»ƒå¤šè¾“å‡ºæ¨¡å‹"""
    if len(X_val) == 0:
        print("è­¦å‘Šï¼šéªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯")
        X_val_tensor = None
        y_val_tensor = None
    else:
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
    
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    
    train_dataset = DataLoader(
        list(zip(X_train_tensor, y_train_tensor)), 
        batch_size=32, shuffle=True
    )
    
    if X_val_tensor is not None:
        val_dataset = DataLoader(
            list(zip(X_val_tensor, y_val_tensor)), 
            batch_size=32
        )
    else:
        val_dataset = None
    
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    early_stopping = EarlyStopping(patience=15)
    
    best_loss = float('inf')
    
    for epoch in range(100):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_dataset:
            optimizer.zero_grad()
            outputs = model(X_batch)
            
            # è®¡ç®—å¤šè¾“å‡ºæŸå¤±
            if model_type == "temperature":
                loss_high = criterion(outputs['high_temp'], y_batch[:, :, 0])  # æœ€é«˜æ¸©
                loss_low = criterion(outputs['low_temp'], y_batch[:, :, 1])    # æœ€ä½æ¸©
                loss_avg = criterion(outputs['avg_temp'], y_batch[:, :, 2])    # å¹³å‡æ¸©
                loss = (loss_high + loss_low + loss_avg) / 3
            else:
                loss = criterion(outputs, y_batch)
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        # éªŒè¯
        if val_dataset is not None:
            model.eval()
            with torch.no_grad():
                val_losses = []
                for X_val_batch, y_val_batch in val_dataset:
                    pred = model(X_val_batch)
                    if model_type == "temperature":
                        val_loss_high = criterion(pred['high_temp'], y_val_batch[:, :, 0])
                        val_loss_low = criterion(pred['low_temp'], y_val_batch[:, :, 1])
                        val_loss_avg = criterion(pred['avg_temp'], y_val_batch[:, :, 2])
                        val_loss = (val_loss_high + val_loss_low + val_loss_avg) / 3
                    else:
                        val_loss = criterion(pred, y_val_batch)
                    val_losses.append(val_loss.item())
                avg_val_loss = sum(val_losses) / len(val_losses)
            
            print(f'Epoch {epoch + 1}, Train Loss: {total_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
            
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                torch.save(model.state_dict(), model_path)
            
            scheduler.step(avg_val_loss)
            early_stopping(avg_val_loss)
            if early_stopping.early_stop:
                print("æ—©åœè§¦å‘ï¼Œç»“æŸè®­ç»ƒ")
                break
        else:
            print(f'Epoch {epoch + 1}, Train Loss: {total_loss:.4f}')
            if total_loss < best_loss:
                best_loss = total_loss
                torch.save(model.state_dict(), model_path)
    
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜åˆ° {model_path}")
    return model

def train_simple_rain_model(model, X_train, y_train, X_val, y_val, model_path, rain_threshold=5.0):
    """è®­ç»ƒç®€åŒ–çš„é™é›¨æ¨¡å‹ - ç›´æ¥é¢„æµ‹é™é›¨é‡"""
    if len(X_val) == 0:
        print("è­¦å‘Šï¼šéªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯")
        X_val_tensor = None
        y_val_tensor = None
    else:
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
    
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    
    train_dataset = DataLoader(
        list(zip(X_train_tensor, y_train_tensor)), 
        batch_size=32, shuffle=True
    )
    
    if X_val_tensor is not None:
        val_dataset = DataLoader(
            list(zip(X_val_tensor, y_val_tensor)), 
            batch_size=32
        )
    else:
        val_dataset = None
    
    # ä½¿ç”¨MSEæŸå¤±å‡½æ•°è¿›è¡Œå›å½’
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    early_stopping = EarlyStopping(patience=15)
    
    best_loss = float('inf')
    
    for epoch in range(100):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_dataset:
            optimizer.zero_grad()
            outputs = model(X_batch)
            
            # ç›´æ¥ä½¿ç”¨MSEæŸå¤±
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        # éªŒè¯
        if val_dataset is not None:
            model.eval()
            with torch.no_grad():
                val_losses = []
                for X_val_batch, y_val_batch in val_dataset:
                    pred = model(X_val_batch)
                    val_loss = criterion(pred, y_val_batch)
                    val_losses.append(val_loss.item())
                avg_val_loss = sum(val_losses) / len(val_losses)
            
            print(f'Epoch {epoch + 1}, Train Loss: {total_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
            
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                torch.save(model.state_dict(), model_path)
            
            scheduler.step(avg_val_loss)
            early_stopping(avg_val_loss)
            if early_stopping.early_stop:
                print("æ—©åœè§¦å‘ï¼Œç»“æŸè®­ç»ƒ")
                break
        else:
            print(f'Epoch {epoch + 1}, Train Loss: {total_loss:.4f}')
            if total_loss < best_loss:
                best_loss = total_loss
                torch.save(model.state_dict(), model_path)
    
    print(f"ç®€åŒ–é™é›¨æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜åˆ° {model_path}")
    return model

if __name__ == "__main__":
    city = input("è¯·è¾“å…¥åŸå¸‚åç§°ï¼ˆå¦‚ è¥¿å®‰ã€åŒ—äº¬ã€ä¸Šæµ·ï¼‰ï¼š")
    train_and_save(city, forecast_days_list=[3, 5])  # å¯æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šå¤©æ•°